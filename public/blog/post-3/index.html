<!DOCTYPE html>
<html lang="en-us"><head>
  <meta charset="utf-8">
  <title>Policy Gradients in a NutShell </title>

  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="This is meta description.">
  <meta name="author" content="Ashutosh Tiwari">
  <meta name="generator" content="Hugo 0.74.3" />

  <style>
    :root{
      --primary-color:#632d17;
    }
  </style>

  <!-- plugins -->
  
  <link rel="stylesheet" href="https://ashutoshtiwari13.github.io/plugins/bootstrap/bootstrap.min.css ">
  
  <link rel="stylesheet" href="https://ashutoshtiwari13.github.io/plugins/slick/slick.css ">
  
  <link rel="stylesheet" href="https://ashutoshtiwari13.github.io/plugins/themify-icons/themify-icons.css ">
  

  <!-- Main Stylesheet -->
  
  <link rel="stylesheet" href="https://ashutoshtiwari13.github.io/css/style.min.css" media="screen">

  <!--Favicon-->
  <link rel="shortcut icon" href="https://ashutoshtiwari13.github.io/images/favicon.png " type="image/x-icon">
  <link rel="icon" href="https://ashutoshtiwari13.github.io/images/favicon.png " type="image/x-icon">


  
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-175538138-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-175538138-1');
</script>

</head>
<body><header class="navigation fixed-top">
  <nav class="navbar navbar-expand-lg navbar-dark">
    <a class="navbar-brand" href="https://ashutoshtiwari13.github.io">
      
      <h4 class="text-white font-tertiary">&lt;Ashutosh Tiwari/&gt;</h4>
      
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navigation"
      aria-controls="navigation" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse text-center" id="navigation">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link" href="https://ashutoshtiwari13.github.io"> Home </a>
        </li>
        
        
        
        
        
        <li class="nav-item ">
          <a class="nav-link" href="https://ashutoshtiwari13.github.io/about">About</a>
        </li>
        
        
        
        
        
        <li class="nav-item ">
          <a class="nav-link" href="https://ashutoshtiwari13.github.io/publication">Publications</a>
        </li>
        
        
        
        
        
        <li class="nav-item ">
          <a class="nav-link" href="https://ashutoshtiwari13.github.io/portfolio">Projects</a>
        </li>
        
        
        
        
        
        <li class="nav-item ">
          <a class="nav-link" href="https://ashutoshtiwari13.github.io/blog">Blog</a>
        </li>
        
        
        
        
        
        <li class="nav-item ">
          <a class="nav-link" href="https://ashutoshtiwari13.github.io/contact">Contact</a>
        </li>
        
        
      </ul>
    </div>
  </nav>
</header>


  
<section class="page-title bg-primary position-relative overflow-hidden">
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h1 class="text-white font-tertiary">Policy Gradients in a NutShell </h1>
      </div>
    </div>
  </div>
  
    <img src="/images/illustrations/main_image.svg" alt="bg-shape" class="bg-shape-4 w-80">
  <img src="/images/illustrations/page-title.png" alt="illustrations" class="bg-shape-1 w-100">
  <img src="/images/illustrations/leaf-pink-round.svg" alt="illustrations" class="bg-shape-2">
  
  <img src="/images/illustrations/leaf-orange.svg" alt="illustrations" class="bg-shape-4">
  <img src="/images/illustrations/leaf-yellow.svg" alt="illustrations" class="bg-shape-5">
  
  <img src="/images/illustrations/leaf-cyan-lg.svg" alt="illustrations" class="bg-shape-7">
</section>



  <section class="section">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-9 text-center mb-4">
          <img src="https://ashutoshtiwari13.github.io/images/blog/post-3.jpg" alt="Policy Gradients in a NutShell " class="img-fluid rounded mb-4">
          <p class="font-secondary">Published on Aug 22, 2020 by <span class="text-primary">Ashutosh Tiwari</span></p>
        </div>
        <div class="col-lg-9">
          <div class="content">
            <h4 id="introduction">Introduction</h4>
<p>Reinforcement Learning (RL) refers to both the learning problem and the sub-field of machine learning which has lately been in the news for great reasons. RL based systems have now <a href="https://deepmind.com/blog/article/alphago-zero-starting-scratch">beaten world champions of Go</a>, helped operate datacenters better and <a href="https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning">mastered a wide variety of Atari games</a>. The research community is seeing many more promising results. With enough motivation, let us now take a look at the Reinforcement Learning problem.</p>
<p>Reinforcement Learning is the most general description of the learning problem where the aim is to maximize a long-term objective. The system description consists of an agent which interacts with the environment via its actions at discrete time steps and receives a reward. This transitions the agent into a new state. A canonical agent-environment feedback loop is depicted by the figure below.</p>
<p><img src="/images/blog/rl-cycle.png" alt="rl-cycle"></p>
<p>The Reinforcement Learning flavor of the learning problem is strikingly similar to how humans effectively behave — experience the world, accumulate knowledge and use the learnings to handle novel situations. Like many people, this attractive nature (although a harder formulation) of the problem is what excites me and hope it does you as well.</p>
<h4 id="policy-gradients">Policy Gradients</h4>
<p>The objective of a Reinforcement Learning agent is to maximize the “expected” reward when following a policy π. Like any Machine Learning setup, we define a set of parameters θ (e.g. the coefficients of a complex polynomial or the weights and biases of units in a neural network) to parametrize this policy — π_θ​ (also written a π for brevity). If we represent the total reward for a given trajectory τ as r(τ), we arrive at the following definition.</p>
<blockquote>
<p>Reinforcement Learning Objective: Maximize the “expected” reward following a parametrized policy</p>
</blockquote>
<p><img src="/images/blog/eq1.png" alt="eq1"></p>
<p>All finite MDPs have at least one optimal policy (which can give the maximum reward) and among all the optimal policies at least one is stationary and deterministic.</p>
<p>Like any other Machine Learning problem, if we can find the parameters θ⋆ which maximize J, we will have solved the task. A standard approach to solving this maximization problem in Machine Learning Literature is to use Gradient Ascent (or Descent). In gradient ascent, we keep stepping through the parameters using the following update rule</p>
<p><img src="/images/blog/eq2.png" alt="eq2"></p>
<p>Here comes the challenge, how do we find the gradient of the objective above which contains the expectation. Integrals are always bad in a computational setting. We need to find a way around them. First step is to reformulate the gradient starting with the expansion of expectation (with a slight abuse of notation).</p>
<p><img src="/images/blog/eq3.png" alt="eq3"></p>
<blockquote>
<p>The Policy Gradient Theorem: The derivative of the expected reward is the expectation of the product of the reward and gradient of the log of the policy π_θ​.</p>
</blockquote>
<p><img src="/images/blog/eq4.png" alt="eq4"></p>
<p>Now, let us expand the definition of π_θ​(τ).</p>
<p><img src="/images/blog/eq5.png" alt="eq5"></p>
<p>To understand this computation, let us break it down — P represents the ergodic distribution of starting in some state s_0​. From then onwards, we apply the product rule of probability because each new action probability is independent of the previous one (remember Markov?). At each step, we take some action using the policy π_θ​ and the environment dynamics p decide which new state to transition into. Those are multiplied over T time steps representing the length of the trajectory. Equivalently, taking the log, we have</p>
<p><img src="/images/blog/eq6.png" alt="eq6"></p>
<p>This result is beautiful in its own right because this tells us, that we don’t really need to know about the ergodic distribution of states P nor the environment dynamics p. This is crucial because for most practical purposes, it hard to model both these variables. Getting rid of them, is certainly good progress. As a result, all algorithms that use this result are known as “Model-Free Algorithms” because we don’t “model” the environment.</p>
<p>The “expectation” (or equivalently an integral term) still lingers around. A simple but effective approach is to sample a large number of trajectories (I really mean LARGE!) and average them out. This is an approximation but an unbiased one, similar to approximating an integral over continuous space with a discrete set of points in the domain. This technique is formally known as Markov Chain Monte-Carlo (MCMC), widely used in Probabilistic Graphical Models and Bayesian Networks to approximate parametric probability distributions.</p>
<p>One term that remains untouched in our treatment above is the reward of the trajectory r(τ). Even though the gradient of the parametrized policy does not depend on the reward, this term adds a lot of variance in the MCMC sampling. Effectively, there are T sources of variance with each R_t​ contributing. However, we can instead make use of the returns G_t​ because from the standpoint of optimizing the RL objective, rewards of the past don’t contribute anything. Hence, if we replace r(τ) by the discounted return G_t​, we arrive at the classic algorithm Policy Gradient algorithm called REINFORCE.</p>

          </div>
        </div>
      </div>
    </div>
  </section>



<section class="section section-on-footer" data-background="https://ashutoshtiwari13.github.io/images/backgrounds/bg-dots.png">
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h2 class="section-title">Want to get in touch 🤝 ?</h2>
      </div>
      <div class="col-lg-8 mx-auto">
        <div class="bg-white rounded text-center p-5 shadow-down">
          <h4 class="mb-80">Drop a Hi 😃 </h4>
          <form action="https://formspree.io/xvowyynq" method="POST" class="row">
            <div class="col-md-6">
              <input type="text" id="name" name="name" placeholder="Full Name" class="form-control px-0 mb-4">
            </div>
            <div class="col-md-6">
              <input type="email" id="email" name="email" placeholder="Email Address" class="form-control px-0 mb-4">
            </div>
            <div class="col-12">
              <textarea name="message" id="message" class="form-control px-0 mb-4"
                placeholder="Type Message Here"></textarea>
            </div>
            <div class="col-lg-6 col-10 mx-auto">
              <button class="btn btn-primary w-100">send</button>
            </div>
          </form>
        </div>
      </div>
    </div>
  </div>
</section>




<footer class="bg-dark footer-section">
  <div class="section">
    <div class="container">
      <div class="row">
        <div class="col-md-4">
          <h5 class="text-light">Email</h5>
          <p class="text-white paragraph-lg font-secondary">reachatashutosh@gmail.com</p>
        </div>
        <div class="col-md-4">
          <h5 class="text-light">Phone</h5>
          <p class="text-white paragraph-lg font-secondary">&#43;91-9845942478</p>
        </div>
        <div class="col-md-4">
          <h5 class="text-light">Address</h5>
          <p class="text-white paragraph-lg font-secondary">Bengaluru, India</p>
        </div>
      </div>
    </div>
  </div>
  <div class="border-top text-center border-dark py-5">
    <p class="mb-0 text-light">Made with ☕️ &amp; ❤️ By Ashutosh Tiwari</p>
  </div>
</footer>


<!-- Google Map API -->

<script src="https://maps.googleapis.com/maps/api/js?key=&amp;#038;types=%28cities%29&amp;#038;libraries=places&amp;#038;language=en"></script>


<!-- JS Plugins -->

<script src="https://ashutoshtiwari13.github.io/plugins/jQuery/jquery.min.js"></script>

<script src="https://ashutoshtiwari13.github.io/plugins/bootstrap/bootstrap.min.js"></script>

<script src="https://ashutoshtiwari13.github.io/plugins/slick/slick.min.js"></script>

<script src="https://ashutoshtiwari13.github.io/plugins/shuffle/shuffle.min.js"></script>

<script src="https://ashutoshtiwari13.github.io/plugins/google-map/map.js"></script>


<!-- Main Script -->

<script src="https://ashutoshtiwari13.github.io/js/script.min.js"></script>
</body>

</html>